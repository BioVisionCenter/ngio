{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NGIO: Streamlined OME-Zarr Image Analysis","text":"<p>ngio is a Python library designed to simplify bioimage analysis workflows, offering an intuitive interface for working with OME-Zarr files.</p>"},{"location":"#what-is-ngio","title":"What is Ngio?","text":"<p>Ngio is built for the OME-Zarr file format, a modern, cloud-optimized format for biological imaging data. OME-Zarr stores large, multi-dimensional microscopy images and metadata in an efficient and scalable way.</p> <p>Ngio's mission is to streamline working with OME-Zarr files by providing a simple, object-based API for opening, exploring, and manipulating OME-Zarr images and high-content screening (HCS) plates. It also offers comprehensive support for labels, tables and regions of interest (ROIs), making it easy to extract and analyze specific regions in your data.</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#simple-object-based-api","title":"\ud83d\udcca Simple Object-Based API","text":"<ul> <li>Easily open, explore, and manipulate OME-Zarr images and HCS plates</li> <li>Create and derive new images and labels with minimal boilerplate code</li> </ul>"},{"location":"#rich-tables-and-regions-of-interest-roi-support","title":"\ud83d\udd0d Rich Tables and Regions of Interest (ROI) Support","text":"<ul> <li>Extract and analyze specific regions of interest</li> <li>Tight integration with Fractal's table framework</li> </ul>"},{"location":"#scalable-data-processing-coming-soon","title":"\ud83d\udd04 Scalable Data Processing (Coming Soon)","text":"<ul> <li>Powerful iterators for processing data at scale</li> <li>Efficient memory management for large datasets</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Refer to the Getting Started guide to integrate ngio into your workflows. We also provide a collection of Tutorials to help you get up and running quickly. For more advanced usage and API documentation, see our API Reference.</p>"},{"location":"#supported-ome-zarr-versions","title":"Supported OME-Zarr versions","text":"<p>Currently, ngio only supports OME-Zarr v0.4. Support for version 0.5 and higher is planned for future releases.</p>"},{"location":"#development-status","title":"Development Status","text":"<p>Warning</p> <p>Ngio is under active development and is not yet stable. The API is subject to change, and bugs and breaking changes are expected. We follow Semantic Versioning. Which means for 0.x releases potentially breaking changes can be introduced in minor releases.</p>"},{"location":"#available-features","title":"Available Features","text":"<ul> <li>\u2705 OME-Zarr metadata handling and validation</li> <li>\u2705 Image and label access across pyramid levels</li> <li>\u2705 ROI and table support</li> <li>\u2705 Streaming from remote sources</li> <li>\u2705 Documentation and examples</li> </ul>"},{"location":"#upcoming-features","title":"Upcoming Features","text":"<ul> <li>Advanced image processing iterators</li> <li>Parallel processing capabilities</li> <li>Support for OME-Zarr v0.5 and Zarr v3</li> </ul>"},{"location":"#contributors","title":"Contributors","text":"<p>Ngio is developed at the BioVisionCenter, University of Zurich, by @lorenzocerrone and @jluethi.</p>"},{"location":"#license","title":"License","text":"<p>Ngio is released under the BSD-3-Clause License. See LICENSE for details.</p>"},{"location":"#repository","title":"Repository","text":"<p>Visit our GitHub repository for the latest code, issues, and contributions.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v032","title":"[v0.3.2]","text":""},{"location":"changelog/#api-changes","title":"API Changes","text":"<ul> <li>change table backend default to <code>anndata_v1</code> for backward compatibility. This will be chaanged again when ngio <code>v0.2.x</code> is no longer supported.</li> </ul>"},{"location":"changelog/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>fix #13 (converters tools)</li> <li>fix #88</li> </ul>"},{"location":"code_of_conduct/","title":"Code of Conduct","text":"<p>Warning</p> <p>The library is still in the early stages of development, the code of conduct is not yet established.</p>"},{"location":"contributing/","title":"Contributing Guide","text":"<p>Warning</p> <p>The library is still in the early stages of development, no contribution guidelines are established yet. But contributions are welcome! Please open an issue or a pull request to discuss your ideas. We are looking for contributors to help us improve the library and documentation.</p>"},{"location":"api/common/","title":"ngio.common API documentation","text":"<p>Warning</p> <p>Coming soon, ngio API documentation is not yet available. If you have any questions please reach out to us on GitHub or via email. We are happy to help you with any questions or issues you may have.</p>"},{"location":"api/hcs/","title":"ngio.hcs API documentation","text":"<p>Warning</p> <p>Coming soon, ngio API documentation is not yet available. If you have any questions please reach out to us on GitHub or via email. We are happy to help you with any questions or issues you may have.</p>"},{"location":"api/images/","title":"ngio.images API documentation","text":"<p>Warning</p> <p>Coming soon, ngio API documentation is not yet available. If you have any questions please reach out to us on GitHub or via email. We are happy to help you with any questions or issues you may have.</p>"},{"location":"api/ngio/","title":"ngio API documentation","text":"<p>Warning</p> <p>Coming soon, ngio API documentation is not yet available. If you have any questions please reach out to us on GitHub or via email. We are happy to help you with any questions or issues you may have.</p>"},{"location":"api/tables/","title":"ngio.tables API documentation","text":"<p>Warning</p> <p>Coming soon, ngio API documentation is not yet available. If you have any questions please reach out to us on GitHub or via email. We are happy to help you with any questions or issues you may have.</p>"},{"location":"api/utils/","title":"ngio.utils","text":"<p>Warning</p> <p>Coming soon, ngio API documentation is not yet available. If you have any questions please reach out to us on GitHub or via email. We are happy to help you with any questions or issues you may have.</p>"},{"location":"getting_started/0_quickstart/","title":"Quickstart","text":"<p>Ngio is a Python package that provides a simple and intuitive API for reading and writing data to and from OME-Zarr. This guide will walk you through the basics of using <code>ngio</code> to read and write data.</p>"},{"location":"getting_started/0_quickstart/#installation","title":"Installation","text":"<p><code>ngio</code> can be installed from PyPI, conda-forge, or from source.</p> <ul> <li><code>ngio</code> requires Python <code>&gt;=3.11</code></li> </ul> pipmamba/condaSource <p>The recommended way to install <code>ngio</code> is from PyPI using pip:</p> <pre><code>pip install ngio\n</code></pre> <p>Alternatively, you can install <code>ngio</code> using mamba:</p> <pre><code>mamba install -c conda-forge ngio\n</code></pre> <p>or conda:</p> <pre><code>conda install -c conda-forge ngio\n</code></pre> <ol> <li> <p>Clone the repository: <pre><code>git clone https://github.com/fractal-analytics-platform/ngio.git\ncd ngio\n</code></pre></p> </li> <li> <p>Install the package: <pre><code>pip install .\n</code></pre></p> </li> </ol>"},{"location":"getting_started/0_quickstart/#troubleshooting","title":"Troubleshooting","text":"<p>Please report installation problems by opening an issue on our GitHub repository.</p>"},{"location":"getting_started/0_quickstart/#setup-some-test-data","title":"Setup some test data","text":"<p>Let's start by downloading a sample OME-Zarr dataset to work with.</p> <pre><code>from pathlib import Path\nfrom ngio.utils import download_ome_zarr_dataset\n\n# Download a sample dataset\ndownload_dir = Path(\"./data\")\nhcs_path = download_ome_zarr_dataset(\"CardiomyocyteSmallMip\", download_dir=download_dir)\nimage_path = hcs_path / \"B\" / \"03\" / \"0\"\n</code></pre>"},{"location":"getting_started/0_quickstart/#open-an-ome-zarr-image","title":"Open an OME-Zarr image","text":"<p>Let's start by opening an OME-Zarr file and inspecting its contents.</p> <pre><code>&gt;&gt;&gt; from ngio import open_ome_zarr_container\n&gt;&gt;&gt; ome_zarr_container = open_ome_zarr_container(image_path)\n&gt;&gt;&gt; ome_zarr_container\nOmeZarrContainer(levels=5, #labels=4, #tables=7)\n</code></pre>"},{"location":"getting_started/0_quickstart/#what-is-the-ome-zarr-container","title":"What is the OME-Zarr container?","text":"<p>The <code>OME-Zarr Container</code> is the core of ngio and the entry point to working with OME-Zarr images. It provides high-level access to the image metadata, images, labels, and tables.</p>"},{"location":"getting_started/0_quickstart/#what-is-the-ome-zarr-container-not","title":"What is the OME-Zarr container not?","text":"<p>The <code>OME-Zarr Container</code> object does not allow the user to interact with the image data directly. For that, we need to use the <code>Image</code>, <code>Label</code>, and <code>Table</code> objects.</p>"},{"location":"getting_started/0_quickstart/#next-steps","title":"Next steps","text":"<p>To learn how to work with the <code>OME-Zarr Container</code> object, but also with the image, label, and table data, check out the following guides:</p> <ul> <li>OME-Zarr Container: An overview on how to use the OME-Zarr Container object and how to create new images and labels.</li> <li>Images/Labels: To know more on how to work with image data.</li> <li>Tables: To know more on how to work with table data, and how you can combine tables with image data.</li> <li>Masked Images/Labels: To know more on how to work with masked image data.</li> <li>HCS Plates: To know more on how to work with HCS plate data.</li> </ul> <p>Also, checkout our jupyer notebook tutorials for more examples:</p> <ul> <li>Image Processing: Learn how to perform simple image processing operations.</li> <li>Image Segmentation: Learn how to create new labels from images.</li> <li>Feature Extraction: Learn how to extract features from images.</li> <li>HCS Processing: Learn how to process high-content screening data using ngio.</li> </ul>"},{"location":"getting_started/1_ome_zarr_containers/","title":"1. OME-Zarr Container","text":"<p>Let's see how to open and explore an OME-Zarr image using <code>ngio</code>:</p> <pre><code>from pathlib import Path\nfrom ngio import open_ome_zarr_container\nfrom ngio.utils import download_ome_zarr_dataset\n\n# Download a sample dataset\ndownload_dir = Path(\"./data\")\nhcs_path = download_ome_zarr_dataset(\"CardiomyocyteSmallMip\", download_dir=download_dir)\nimage_path = hcs_path / \"B\" / \"03\" / \"0\"\n\n# Open the OME-Zarr container\nome_zarr_container = open_ome_zarr_container(image_path)\n</code></pre> <p>The <code>OME-Zarr Container</code> in is your entry point to working with OME-Zarr images. It provides high-level access to the image metadata, images, labels, and tables.</p> <pre><code>&gt;&gt;&gt; ome_zarr_container\nOmeZarrContainer(levels=5, #labels=4, #tables=7)\n</code></pre> <p>The <code>OME-Zarr Container</code> will be the starting point for all your image processing tasks.</p>"},{"location":"getting_started/1_ome_zarr_containers/#main-concepts","title":"Main concepts","text":""},{"location":"getting_started/1_ome_zarr_containers/#what-is-the-ome-zarr-container","title":"What is the OME-Zarr container?","text":"<p>The <code>OME-Zarr Container</code> in ngio is your entry point to working with OME-Zarr images.</p> <p>It provides:</p> <ul> <li>OME-Zarr overview: get an overview of the OME-Zarr file, including the number of image levels, list of labels, and tables available.</li> <li>Image access: get access to the images at different resolution levels and pixel sizes.</li> <li>Label management: check which labels are available, access them, and create new labels.</li> <li>Table management: check which tables are available, access them, and create new tables.</li> <li>Derive new OME-Zarr images: create new images based on the original one, with the same or similar metadata.</li> </ul>"},{"location":"getting_started/1_ome_zarr_containers/#what-is-the-ome-zarr-container-not","title":"What is the OME-Zarr container not?","text":"<p>The <code>OME-Zarr Container</code> object does not allow the user to interact with the image data directly. For that, we need to use the <code>Image</code>, <code>Label</code>, and <code>Table</code> objects.</p>"},{"location":"getting_started/1_ome_zarr_containers/#ome-zarr-overview","title":"OME-Zarr overview","text":"<p>Examples of the OME-Zarr metadata access:</p> Number of Resolution LevelsAvailable PathsDimensionalityFull Metadata Object <p>Show the number of resolution levels: <pre><code>&gt;&gt;&gt; ome_zarr_container.levels # Show the number of resolution levels\n5\n</code></pre></p> <p>Show the paths to all available resolution levels: <pre><code>&gt;&gt;&gt; ome_zarr_container.levels_paths # Show the paths to all available images\n['0', '1', '2', '3', '4']\n</code></pre></p> <p>Show if the image is 2D or 3D: <pre><code>&gt;&gt;&gt; ome_zarr_container.is_3d # Get if the image is 3D\nFalse\n</code></pre> or if the image is a time series: <pre><code>&gt;&gt;&gt; ome_zarr_container.is_time_series # Get if the image is a time series\nFalse\n</code></pre></p> <p><pre><code>&gt;&gt;&gt; metadata = ome_zarr_container.image_meta\nNgioImageMeta(name=None, datasets=['0', '1', '2', '3', '4'], axes=['c', 'z', 'y', 'x'])\n</code></pre> The metadata object contains all the information about the image, for example, the channel labels: <pre><code>&gt;&gt;&gt; metadata.channel_labels\n['DAPI', 'nanog', 'Lamin B1']\n</code></pre></p>"},{"location":"getting_started/1_ome_zarr_containers/#accessing-images-labels-tables","title":"Accessing images / labels / tables","text":"<p>To access images, labels, and tables, you can use the <code>get_image</code>, <code>get_label</code>, and <code>get_table</code> methods of the <code>OME-Zarr Container</code> object.</p> <p>A variety of examples and additional information can be found in the Images and Labels, and Tables sections.</p>"},{"location":"getting_started/1_ome_zarr_containers/#creating-derived-images","title":"Creating derived images","text":"<p>When processing an image, you might want to create a new image with the same metadata:</p> <pre><code># Create a new image based on the original\nnew_image = ome_zarr_container.derive_image(\"data/new_ome.zarr\", overwrite=True)\n</code></pre> <p>This will create a new OME-Zarr image with the same metadata as the original image. But you can also create a new image with slightly different metadata, for example, with a different shape:</p> <pre><code># Create a new image with a different shape\nnew_image = ome_zarr_container.derive_image(\n    \"data/new_ome.zarr\", \n    overwrite=True, \n    shape=(16, 128, 128), \n    xy_pixelsize=0.65, \n    z_spacing=1.0\n)\n</code></pre>"},{"location":"getting_started/1_ome_zarr_containers/#creating-new-images","title":"Creating new images","text":"<p>You can create OME-Zarr images from an existing numpy array using the <code>create_ome_zarr_from_array</code> function.</p> <pre><code>import numpy as np\nfrom ngio import create_ome_zarr_from_array\n\n# Create a random 3D array\nx = np.random.randint(0, 255, (16, 128, 128), dtype=np.uint8)\n\n# Save as OME-Zarr\nnew_ome_zarr_image = create_ome_zarr_from_array(\n    store=\"random_ome.zarr\", \n    array=x, \n    xy_pixelsize=0.65, \n    z_spacing=1.0\n)\n</code></pre> <p>Alternatively, if you wanto to create an a empty OME-Zarr image, you can use the <code>create_empty_ome_zarr</code> function:</p> <pre><code>from ngio import create_empty_ome_zarr\n# Create an empty OME-Zarr image\nnew_ome_zarr_image = create_empty_ome_zarr(\n    store=\"empty_ome.zarr\", \n    shape=(16, 128, 128), \n    xy_pixelsize=0.65, \n    z_spacing=1.0\n)\n</code></pre> <p>This will create an empty OME-Zarr image with the specified shape and pixel sizes.</p>"},{"location":"getting_started/1_ome_zarr_containers/#opening-remote-ome-zarr-containers","title":"Opening remote OME-Zarr containers","text":"<p>You can use <code>ngio</code> to open remote OME-Zarr containers. For publicly available OME-Zarr containers, you can just use the <code>open_ome_zarr_container</code> function with a URL.</p> <p>For example, to open a remote OME-Zarr container hosted on a github repository:</p> <pre><code>from ngio.utils import fractal_fsspec_store\n\nurl = (\n    \"https://raw.githubusercontent.com/\"\n    \"fractal-analytics-platform/fractal-ome-zarr-examples/\"\n    \"refs/heads/main/v04/\"\n    \"20200812-CardiomyocyteDifferentiation14-Cycle1_B_03_mip.zarr/\"\n)\n\nstore = fractal_fsspec_store(url=url)\nome_zarr_container = open_ome_zarr_container(store)\n</code></pre> <p>For fractal users, the <code>fractal_fsspec_store</code> function can be used to open private OME-Zarr containers. In this case we need to provide a <code>fractal_token</code> to authenticate the user.</p> <pre><code>from ngio.utils import fractal_fsspec_store\n\nstore = fractal_fsspec_store(url=\"https://fractal_url...\", fractal_token=\"**your_secret_token**\")\nome_zarr_container = open_ome_zarr_container(store)\n</code></pre>"},{"location":"getting_started/2_images/","title":"2. Images and Labels","text":""},{"location":"getting_started/2_images/#images","title":"Images","text":"<p>In order to start working with the image data, we need to instantiate an <code>Image</code> object. ngio provides a high-level API to access the image data at different resolution levels and pixel sizes.</p>"},{"location":"getting_started/2_images/#getting-an-image","title":"Getting an image","text":"Highest Resolution ImageSpecific Pyramid LevelSpecific ResolutionNearest Resolution <p>By default, the <code>get_image</code> method returns the highest resolution image: <pre><code>&gt;&gt;&gt; ome_zarr_container.get_image() # Get the highest resolution image\nImage(path=0, Dimensions(c: 3, z: 1, y: 4320, x: 5120))\n</code></pre></p> <p>To get a specific pyramid level, you can use the <code>path</code> parameter: <pre><code>&gt;&gt;&gt; ome_zarr_container.get_image(path=\"1\") # Get a specific pyramid level\nImage(path=1, Dimensions(c: 3, z: 1, y: 2160, x: 2560))\n</code></pre> This will return the image at the specified pyramid level.</p> <p>If you want to get an image with a specific pixel size, you can use the <code>pixel_size</code> parameter: <pre><code>&gt;&gt;&gt; from ngio import PixelSize\n&gt;&gt;&gt; pixel_size = PixelSize(x=0.65, y=0.65, z=1.0)\n&gt;&gt;&gt; ome_zarr_container.get_image(pixel_size=pixel_size)\nImage(path=2, Dimensions(c: 3, z: 1, y: 1080, x: 1280))\n</code></pre></p> <p>By default the pixels must match exactly the requested pixel size. If you want to get the nearest resolution, you can use the <code>strict</code> parameter: <pre><code>&gt;&gt;&gt; from ngio import PixelSize\n&gt;&gt;&gt; pixel_size = PixelSize(x=0.60, y=0.60, z=1.0)\n&gt;&gt;&gt; ome_zarr_container.get_image(pixel_size=pixel_size, strict=False)\nImage(path=2, Dimensions(c: 3, z: 1, y: 1080, x: 1280))\n</code></pre> This will return the image with the nearest resolution to the requested pixel size.</p> <p>Similarly to the <code>OME-Zarr Container</code>, the <code>Image</code> object provides a high-level API to access the image metadata.</p> DimensionsPixel SizeOn disk array infos <p><pre><code>&gt;&gt;&gt; image.dimensions\nDimensions(c: 3, z: 1, y: 1080, x: 1280)\n</code></pre> The <code>dimensions</code> attribute returns a object with the image dimensions for each axis.</p> <p><pre><code>&gt;&gt;&gt; image.pixel_size\nPixelSize(x=0.65, y=0.65, z=1.0, t=1.0)\n</code></pre> The <code>pixel_size</code> attribute returns the pixel size for each axis.</p> <p><pre><code>&gt;&gt;&gt; image.shape, image.dtype, image.chunks\n(3, 1, 1080, 1280) uint16 (1, 1, 1080, 1280)\n</code></pre> The <code>axes</code> attribute returns the order of the axes in the image.</p>"},{"location":"getting_started/2_images/#working-with-image-data","title":"Working with image data","text":"<p>Once you have the <code>Image</code> object, you can access the image data as a:</p> Numpy ArrayDask ArrayDask Delayed <pre><code>&gt;&gt;&gt; data = image.get_array() # Get the image as a numpy array\n&gt;&gt;&gt; data.shape, data.dtype\n(3, 1, 1080, 1280) uint16\n</code></pre> <pre><code>&gt;&gt;&gt; dask_array = image.get_array(mode=\"dask\") # Get the image as a dask array\n&gt;&gt;&gt; dask_array\ndask.array&lt;from-zarr, shape=(3, 1, 1080, 1280), dtype=uint16, chunksize=(1, 1, 1080, 1280), chunktype=numpy.ndarray&gt;\n</code></pre> <pre><code>&gt;&gt;&gt; dask_delayed = image.get_array(mode=\"delayed\") # Get the image as a dask delayed object\n&gt;&gt;&gt; dask_delayed\nDelayed('transform_numpy_array-b84d3da4-34fa-4874-9fed-8c4430e17db4')\n</code></pre> <p>The <code>get_array</code> can also be used to slice the image data, and query specific axes in specific orders:</p> <pre><code>&gt;&gt;&gt; image_slice = image.get_array(c=0, x=slice(0, 128), axes_order=[\"t\", \"z\", \"y\", \"x\", \"c\"]) # Get a specific channel and axes order\n&gt;&gt;&gt; image_slice.shape\n(1, 1, 1080, 128, 1)\n</code></pre> <p>If you want to edit the image data, you can use the <code>set_array</code> method:</p> <pre><code>&gt;&gt;&gt; image.set_array(data) # Set the image data\n</code></pre> <p>The <code>set_array</code> method can be used to set the image data from a numpy array, dask array, or dask delayed object.</p> <p>A minimal example of how to use the <code>get_array</code> and <code>set_array</code> methods:</p> <pre><code># Get the image data as a numpy array\ndata = image.get_array(c=0, x=slice(0, 128), y=slice(0, 128), axes_order=[\"z\", \"y\", \"x\", \"c\"])\n\n# Modify the image data\ndata = some_function(data)\n\n# Set the modified image data\nimage.set_array(data, c=0, x=slice(0, 128), y=slice(0, 128), axes_order=[\"z\", \"y\", \"x\", \"c\"])\nimage.consolidate() # Consolidate the changes to all resolution levels, see below for more details\n</code></pre> <p>Important</p> <p>The <code>set_array</code> method will overwrite the image data at single resolution level. After you have finished editing the image data, you need to <code>consolidate</code> the changes to the OME-Zarr file at all resolution levels: <pre><code>&gt;&gt;&gt; image.consolidate() # Consolidate the changes\n</code></pre> This will write the changes to the OME-Zarr file at all resolution levels.</p>"},{"location":"getting_started/2_images/#labels","title":"Labels","text":"<p><code>Labels</code> represent segmentation masks that identify objects in the image. In ngio <code>Labels</code> are similar to <code>Images</code> and can be accessed and manipulated in the same way.</p>"},{"location":"getting_started/2_images/#getting-a-label","title":"Getting a label","text":"<p>Now let's see what labels are available in our image:</p> <pre><code>&gt;&gt;&gt; ome_zarr_container.list_labels() # Available labels\n['nuclei', 'wf_2_labels', 'wf_3_labels', 'wf_4_labels']\n</code></pre> <p>We have <code>4</code> labels available in our image. Let's see how to access them:</p> Highest Resolution LabelSpecific Pyramid LevelSpecific ResolutionNearest Resolution <p>By default, the <code>get_label</code> method returns the highest resolution label: <pre><code>&gt;&gt;&gt; ome_zarr_container.get_label(\"nuclei\") # Get the highest resolution label\nLabel(path=0, Dimensions(z: 1, y: 4320, x: 5120))\n</code></pre></p> <p>To get a specific pyramid level, you can use the <code>path</code> parameter: <pre><code>&gt;&gt;&gt; ome_zarr_container.get_label(\"nuclei\", path=\"1\") # Get a specific pyramid level\nLabel(path=1, Dimensions(z: 1, y: 2160, x: 2560))\n</code></pre> This will return the label at the specified pyramid level.</p> <p>If you want to get a label with a specific pixel size, you can use the <code>pixel_size</code> parameter: <pre><code>&gt;&gt;&gt; from ngio import PixelSize\n&gt;&gt;&gt; pixel_size = PixelSize(x=0.65, y=0.65, z=1.0)\n&gt;&gt;&gt; ome_zarr_container.get_label(\"nuclei\", pixel_size=pixel_size)\nLabel(path=2, Dimensions(z: 1, y: 1080, x: 1280))\n</code></pre></p> <p>By default the pixels must match exactly the requested pixel size. If you want to get the nearest resolution, you can use the <code>strict</code> parameter: <pre><code>&gt;&gt;&gt; from ngio import PixelSize\n&gt;&gt;&gt; pixel_size = PixelSize(x=0.60, y=0.60, z=1.0)\n&gt;&gt;&gt; ome_zarr_container.get_label(\"nuclei\", pixel_size=pixel_size, strict=False)\nLabel(path=2, Dimensions(z: 1, y: 1080, x: 1280))\n</code></pre> This will return the label with the nearest resolution to the requested pixel size.</p>"},{"location":"getting_started/2_images/#working-with-label-data","title":"Working with label data","text":"<p>Data access and manipulation for <code>Labels</code> is similar to <code>Images</code>. You can use the <code>get_array</code> and <code>set_array</code> methods to access and modify the label data.</p>"},{"location":"getting_started/2_images/#deriving-a-label","title":"Deriving a label","text":"<p>Often, you might want to create a new label based on an existing image. You can do this using the <code>derive_label</code> method:</p> <pre><code>&gt;&gt;&gt; new_label = ome_zarr_container.derive_label(\"new_label\", overwrite=True) # Derive a new label\nLabel(path=0, Dimensions(z: 1, y: 4320, x: 5120))\n</code></pre> <p>This will create a new label with the same dimensions as the original image (without channels) and compatible metadata. If you want to create a new label with slightly different metadata see API Reference.</p>"},{"location":"getting_started/3_tables/","title":"3. Tables","text":"<p>Tables are not part of the core OME-Zarr specification but can be used in ngio to store measurements, features, regions of interest (ROIs), and other tabular data. Ngio follows the Fractal's Table Spec.</p>"},{"location":"getting_started/3_tables/#getting-a-table","title":"Getting a table","text":"<p>We can list all available tables and load a specific table:</p> <pre><code>&gt;&gt;&gt; ome_zarr_container.list_tables()\n['FOV_ROI_table', 'nuclei_ROI_table', 'well_ROI_table', 'regionprops_DAPI', 'nuclei_measurements_wf3', 'nuclei_measurements_wf4', 'nuclei_lamin_measurements_wf4']\n</code></pre> <p>Ngio supports three types of tables: <code>roi_table</code>, <code>feature_table</code>, and <code>masking_roi_table</code>, as well as untyped <code>generic_table</code>.</p> ROI TableMasking ROI TableFeatures Table <p>ROI tables can be used to store arbitrary regions of interest (ROIs) in the image. Here for example we will load the <code>FOV_ROI_table</code> that contains the microscope field of view (FOV) ROIs: <pre><code>&gt;&gt;&gt; roi_table = ome_zarr_container.get_table(\"FOV_ROI_table\") # Get a ROI table\n&gt;&gt;&gt; roi_table.get(\"FOV_1\")\nname='FOV_1' x_length=416.0 y_length=351.0 z_length=1.0 x=0.0 y=0.0 z=0.0 x_micrometer_original=-1448.300048828125 y_micrometer_original=-1517.699951171875\n</code></pre> 2025-06-23T16:47:53.252665 image/svg+xml Matplotlib v3.10.3, https://matplotlib.org/    This will return all the ROIs in the table. ROIs can be used to slice the image data: <pre><code>&gt;&gt;&gt; roi = roi_table.get(\"FOV_1\")\n&gt;&gt;&gt; roi_data = image.get_roi(roi)\n&gt;&gt;&gt; roi_data.shape\n(3, 1, 540, 640)\n</code></pre> This will return the image data for the specified ROI.  2025-06-23T16:47:53.415775 image/svg+xml Matplotlib v3.10.3, https://matplotlib.org/ </p> <p>Masking ROIs are a special type of ROIs that can be used to store ROIs for masked objects in the image. The <code>nuclei_ROI_table</code> contains the masks for the <code>nuclei</code> label in the image, and is indexed by the label id. <pre><code>&gt;&gt;&gt; masking_table = ome_zarr_container.get_table(\"nuclei_ROI_table\") # Get a mask table\n&gt;&gt;&gt; masking_table.get(1)\nname='100' x_length=11.375 y_length=14.625 z_length=1.0 x=33.63750076293945 y=18.850000381469727 z=0.0\n</code></pre> ROIs can be used to slice the image data: <pre><code>&gt;&gt;&gt; roi = masking_table.get(100)\n&gt;&gt;&gt; roi_data = image.get_roi(roi)\n&gt;&gt;&gt; roi_data.shape\n(3, 1, 22, 18)\n</code></pre> This will return the image data for the specified ROI.  2025-06-23T16:47:53.609151 image/svg+xml Matplotlib v3.10.3, https://matplotlib.org/    See 4. Masked Images and Labels for more details on how to use the masking ROIs to load masked data.</p> <p>Features tables are used to store measurements and are indexed by the label id <pre><code>&gt;&gt;&gt; feature_table = ome_zarr_container.get_table(\"regionprops_DAPI\") # Get a feature table\n&gt;&gt;&gt; feature_table.dataframe.head(5) # only show the first 5 rows\n</code></pre> label area bbox_area equivalent_diameter max_intensity mean_intensity min_intensity standard_deviation_intensity 1 2120 2655 15.9384 476 278.636 86 54.3438 2 327 456 8.54771 604 324.162 118 90.8471 3 1381 1749 13.8165 386 212.682 60 50.1696 4 2566 3588 16.9858 497 251.731 61 53.3072 5 4201 5472 20.0194 466 223.863 51 56.719 </p>"},{"location":"getting_started/3_tables/#creating-a-table","title":"Creating a table","text":"<p>Tables (differently from Images and Labels) can be purely in memory objects, and don't need to be saved on disk.</p> Creating a ROI TableCreating a Masking ROI TableCreating a Feature TableCreating a Generic Table <p><pre><code>&gt;&gt;&gt; from ngio.tables import RoiTable\n&gt;&gt;&gt; from ngio import Roi\n&gt;&gt;&gt; roi = Roi(x=0, y=0, x_length=128, y_length=128, name=\"FOV_1\")\n&gt;&gt;&gt; roi_table = RoiTable(rois=[roi])\nRoiTableV1(num_rois=1)\n</code></pre> If you would like to create on-the-fly a ROI table for the whole image: <pre><code>&gt;&gt;&gt; roi_table = ome_zarr_container.build_image_roi_table(\"whole_image\")\n&gt;&gt;&gt; roi_table\nRoiTableV1(num_rois=1)\n</code></pre> The <code>build_image_roi_table</code> method will create a ROI table with a single ROI that covers the whole image. This table is not associated with the image and is purely in memory. If we want to save it to disk, we can use the <code>add_table</code> method: <pre><code>&gt;&gt;&gt; ome_zarr_container.add_table(\"new_roi_table\", roi_table, overwrite=True)\n&gt;&gt;&gt; roi_table = ome_zarr_container.get_table(\"new_roi_table\")\nRoiTableV1(num_rois=1)\n</code></pre></p> <p>Similarly to the ROI table, we can create a masking ROI table on-the-fly: Let's for example create a masking ROI table for the <code>nuclei</code> label: <pre><code>&gt;&gt;&gt; masking_table = ome_zarr_container.build_masking_roi_table(\"nuclei\")\n&gt;&gt;&gt; masking_table\nMaskingRoiTableV1(num_rois=3006, reference_label=nuclei)\n</code></pre></p> <p>Feature tables can be created from a pandas <code>Dataframe</code>: <pre><code>&gt;&gt;&gt; from ngio.tables import FeatureTable\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; example_data = pd.DataFrame({\"label\": [1, 2, 3], \"area\": [100, 200, 300]})\n&gt;&gt;&gt; feature_table = FeatureTable(table_data=example_data)\n&gt;&gt;&gt; feature_table\nFeatureTableV1(num_rows=3, num_columns=1)\n</code></pre></p> <p>Sometimes you might want to create a table that doesn't fit into the <code>ROI</code>, <code>Masking ROI</code>, or <code>Feature</code> categories. In this case, you can use the <code>GenericTable</code> class, which allows you to store any tabular data. It can be created from a pandas <code>Dataframe</code>: <pre><code>&gt;&gt;&gt; from ngio.tables import GenericTable\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; example_data = pd.DataFrame({\"area\": [100, 200, 300], \"perimeter\": [50, 60, 70]})\n&gt;&gt;&gt; generic_table = GenericTable(table_data=example_data)\n&gt;&gt;&gt; generic_table\nGenericTable\n</code></pre> Or from an \"AnnData\" object: <pre><code>&gt;&gt;&gt; from ngio.tables import GenericTable\n&gt;&gt;&gt; import anndata as ad\n&gt;&gt;&gt; adata = ad.AnnData(X=np.random.rand(10, 5), obs=pd.DataFrame({\"cell_type\": [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\"]}))\n&gt;&gt;&gt; generic_table = GenericTable(table_data=adata)\n&gt;&gt;&gt; generic_table\nGenericTable\n</code></pre> The <code>GenericTable</code> class allows you to store any tabular data, and is a flexible way to work with tables in ngio.</p>"},{"location":"getting_started/4_masked_images/","title":"4. Masked Images and Labels","text":"<p>Masked images (or labels) are images that are masked by an instance segmentation mask.</p> <p>In this section we will show how to create a <code>MaskedImage</code> object and how to use it to get the data of the image.</p> <p></p> <p>Similar to the <code>Image</code> and <code>Label</code> objects, the <code>MaskedImage</code> can be initialized from an <code>OME-Zarr Container</code> object using the <code>get_masked_image</code> method.</p> <p>Let's create a masked image from the <code>nuclei</code> label:</p> <pre><code>&gt;&gt;&gt; masked_image = ome_zarr_container.get_masked_image(\"nuclei\")\n&gt;&gt;&gt; masked_image\nMaskedImage(path=0, Dimensions(c: 3, z: 1, y: 4320, x: 5120), nuclei)\n</code></pre> <p>Since the <code>MaskedImage</code> is a subclass of <code>Image</code>, we can use all the methods available for <code>Image</code> objects.</p> <p>The two most notable exceptions are the <code>get_roi</code> and <code>set_roi</code> which now instead of requiring a <code>roi</code> object, require an integer <code>label</code>.</p> <pre><code>&gt;&gt;&gt; roi_data = masked_image.get_roi(label=1009, c=0)\n&gt;&gt;&gt; roi_data.shape\n(1, 1, 76, 83)\n</code></pre> 2025-06-23T16:47:55.179715 image/svg+xml Matplotlib v3.10.3, https://matplotlib.org/ <p>Additionally we can used the <code>zoom_factor</code> argument to get more context around the ROI. For example we can zoom out the ROI by a factor of <code>2</code>:</p> <pre><code>&gt;&gt;&gt; roi_data = masked_image.get_roi(label=1009, c=0, zoom_factor=2)\n&gt;&gt;&gt; roi_data.shape\n(1, 1, 152, 166)\n</code></pre> 2025-06-23T16:47:55.246746 image/svg+xml Matplotlib v3.10.3, https://matplotlib.org/"},{"location":"getting_started/4_masked_images/#masked-operations","title":"Masked operations","text":"<p>In addition to the <code>get_roi</code> method, the <code>MaskedImage</code> class also provides a masked operation method that allows you to perform reading and writing only on the masked pixels.</p> <p>For these operations we can use the <code>get_roi_masked</code> and <code>set_roi_masked</code> methods. For example, we can use the <code>get_roi_masked</code> method to get the masked data for a specific label:</p> <pre><code>&gt;&gt;&gt; masked_roi_data = masked_image.get_roi_masked(label=1009, c=0, zoom_factor=2)\n&gt;&gt;&gt; masked_roi_data.shape\n(1, 1, 152, 166)\n</code></pre> 2025-06-23T16:47:55.358565 image/svg+xml Matplotlib v3.10.3, https://matplotlib.org/ <p>We can also use the <code>set_roi_masked</code> method to set the masked data for a specific label:</p> <pre><code>&gt;&gt;&gt; masked_data = masked_image.get_roi_masked(label=1009, c=0)\n&gt;&gt;&gt; masked_data = np.random.randint(0, 255, masked_data.shape, dtype=np.uint8)\n&gt;&gt;&gt; masked_image.set_roi_masked(label=1009, c=0, patch=masked_data)\n</code></pre> 2025-06-23T16:47:55.464066 image/svg+xml Matplotlib v3.10.3, https://matplotlib.org/"},{"location":"getting_started/4_masked_images/#masked-labels","title":"Masked Labels","text":"<p>The <code>MaskedLabel</code> class is a subclass of <code>Label</code> and provides the same functionality as the <code>MaskedImage</code> class.</p> <p>The <code>MaskedLabel</code> class can be used to create a masked label from an <code>OME-Zarr Container</code> object using the <code>get_masked_label</code> method.</p> <pre><code>&gt;&gt;&gt; masked_label = ome_zarr_container.get_masked_label(label_name = \"wf_2_labels\", masking_label_name = \"nuclei\")\n&gt;&gt;&gt; masked_label\nMaskedLabel(path=0, Dimensions(z: 1, y: 4320, x: 5120), nuclei)\n</code></pre>"},{"location":"getting_started/5_hcs/","title":"5. HCS Plates","text":"<p>Ngio provides a simple interface for high-content screening (HCS) plates. An HCS plate is a collection of OME-Zarr images organized in a grid-like structure. Each plates contains columns and rows, and each well in the plate is identified by its row and column indices. Each well can contain multiple images, and each image can belong to a different acquisition.</p> <p>The HCS plate is represented by the <code>OmeZarrPlate</code> class.</p> <p>Let's open an <code>OmeZarrPlate</code> object.</p> <pre><code>&gt;&gt;&gt; from ngio.utils import download_ome_zarr_dataset\n&gt;&gt;&gt; from ngio import open_ome_zarr_plate\n&gt;&gt;&gt; hcs_path = download_ome_zarr_dataset(\"CardiomyocyteSmallMip\", download_dir=download_dir)\n&gt;&gt;&gt; ome_zarr_plate = open_ome_zarr_plate(hcs_path)\n&gt;&gt;&gt; ome_zarr_plate\nPlate([rows x columns] (1 x 1))\n</code></pre> <p>This example plate is very small and contains only a single well.</p>"},{"location":"getting_started/5_hcs/#plate-overview","title":"Plate overview","text":"<p>The <code>OmeZarrPlate</code> object provides a high-level overview of the plate, including rows, columns, and acquisitions. The following methods are available:</p> ColumnsRowsAcquisitions <p>Show the columns in the plate: <pre><code>&gt;&gt;&gt; ome_zarr_plate.columns\n['03']\n</code></pre></p> <p>Show the rows in the plate: <pre><code>&gt;&gt;&gt; ome_zarr_plate.rows\n['B']\n</code></pre></p> <p>Show the acquisitions ids: <pre><code>&gt;&gt;&gt; ome_zarr_plate.acquisition_ids\n[0]\n</code></pre></p>"},{"location":"getting_started/5_hcs/#retrieving-the-path-to-the-images","title":"Retrieving the path to the images","text":"<p>The <code>OmeZarrPlate</code> object provides multiple methods to retrieve the path to the images in the plate.</p> All Images PathsAll Wells PathsAll Images Paths in a Well <p>This will return the paths to all images in the plate: <pre><code>&gt;&gt;&gt; ome_zarr_plate.images_paths()\n['B/03/0']\n</code></pre></p> <p>This will return the paths to all wells in the plate: <pre><code>&gt;&gt;&gt; ome_zarr_plate.wells_paths()\n['B/03']\n</code></pre></p> <p>This will return the paths to all images in a well: <pre><code>&gt;&gt;&gt; ome_zarr_plate.well_images_paths(row=\"B\", column=3)\n['B/03/0']\n</code></pre></p>"},{"location":"getting_started/5_hcs/#getting-the-images","title":"Getting the images","text":"<p>The <code>OmeZarrPlate</code> object provides a method to get the image objects in a well. The method <code>get_well_images</code> takes the row and column indices of the well and returns a list of <code>OmeZarrContainer</code> objects.</p> All ImagesAll Images in a WellSpecific ImageFilter by Acquisition <p>Get all images in the plate: <pre><code>&gt;&gt;&gt; ome_zarr_plate.get_images()\n&gt;&gt;&gt; ome_zarr_plate\n{'B/03/0': OmeZarrContainer(levels=5, #labels=4, #tables=7)}\n</code></pre> This dictionary contains the path to the images and the corresponding <code>OmeZarrContainer</code> object.</p> <p>Get all images in a well: <pre><code>&gt;&gt;&gt; well_images = ome_zarr_plate.get_well_images(row=\"B\", column=3)\n&gt;&gt;&gt; well_images\n{'B/03/0': OmeZarrContainer(levels=5, #labels=4, #tables=7)}\n</code></pre> This dictionary contains the path to the images and the corresponding <code>OmeZarrContainer</code> object.</p> <p>Get a specific image in a well: <pre><code>&gt;&gt;&gt; ome_zarr_plate.get_image(row=\"B\", column=3, image_path=\"0\")\nOmeZarrContainer(levels=5, #labels=4, #tables=7)\n</code></pre> This will return the <code>OmeZarrContainer</code> object for the image in the well.</p> <p>In these methods, you can also filter the images by acquisition. When available, the <code>acquisition</code> parameter can be used to filter the images by acquisition id. <pre><code>&gt;&gt;&gt; well_images = ome_zarr_plate.get_well_images(row=\"B\", column=3, acquisition=0)\n&gt;&gt;&gt; well_images\n{}\n</code></pre> The <code>acquisition</code> is not required, and if not provided, an empty dictionary will be returned.</p>"},{"location":"getting_started/5_hcs/#creating-a-plate","title":"Creating a plate","text":"<p>Ngio provides a utility function to create a plate.</p> <p>The first step is to create a list of <code>ImageInWellPath</code> objects. Each <code>ImageInWellPath</code> object contains the path to the image and the corresponding well.</p> <pre><code>from ngio import ImageInWellPath\nlist_of_images = [ImageInWellPath(path=\"0\", row=\"A\", column=0),\n                    ImageInWellPath(path=\"0\", row=\"B\", column=1),\n                    ImageInWellPath(path=\"0\", row=\"C\", column=1),\n                    ImageInWellPath(path=\"1\", row=\"A\", column=0, acquisition_id=1, acquisition_name=\"acquisition_1\"),\n]\n</code></pre> <p>Note</p> <p>The order in which the images are added is not important. The <code>rows</code> and <code>columns</code> attributes of the plate will be sorted in alphabetical/numerical order.</p> <p>Then, you can create the plate using the <code>create_empty_plate</code> function.</p> <pre><code>&gt;&gt;&gt; from ngio import create_empty_plate\n&gt;&gt;&gt; plate = create_empty_plate(store=\"new_plate.zarr\", name=\"test_plate\", images=list_of_images, overwrite=True)\n&gt;&gt;&gt; plate\nPlate([rows x columns] (3 x 2))\n</code></pre> <p>This has created a new empty plate with the metadata correctly set. But no images have been added yet.</p>"},{"location":"getting_started/5_hcs/#modifying-the-plate","title":"Modifying the plate","text":"<p>You can add images or remove images</p> Add ImagesRemove Images <p>To add images to the plate, you can use the <code>add_image</code> method. This method takes the row and column indices of the well and the path to the image. <pre><code>&gt;&gt;&gt; print(f\"Before adding images: {plate.rows} rows, {plate.columns} columns\")\n&gt;&gt;&gt; plate.add_image(row=\"D\", column=0, image_path=\"0\")\n&gt;&gt;&gt; print(f\"After adding images: {plate.rows} rows, {plate.columns} columns\")\nBefore adding images: ['A', 'B', 'C'] rows, ['0', '1'] columns\nAfter adding images: ['A', 'B', 'C', 'D'] rows, ['0', '1'] columns\n</code></pre> This will add a new image to the plate and well metadata.</p> <p>Note</p> <p>The order in which the images are added is not important. The <code>rows</code> and <code>columns</code> attributes of the plate will be sorted in alphabetical/numerical order.</p> <p>Warning</p> <p>This function is not multiprocessing safe. If you are using multiprocessing, you should use the <code>atomic_add_image</code> method instead.</p> <p>To remove images from the plate, you can use the <code>remove_image</code> method. This method takes the row and column indices of the well and the path to the image. <pre><code>&gt;&gt;&gt; print(f\"Before removing images: {plate.wells_paths()} wells\")\n&gt;&gt;&gt; plate.remove_image(row=\"D\", column=0, image_path=\"0\")\n&gt;&gt;&gt; print(f\"After removing images: {plate.wells_paths()} wells\")\nBefore removing images: ['A/0', 'B/1', 'C/1', 'D/0'] wells\nAfter removing images: ['A/0', 'B/1', 'C/1'] wells\n</code></pre> This will remove the image metadata from the plate and well metadata.</p> <p>Warning</p> <p>No data will be removed from the store. If an image is saved in the store it will remain there. Also the metadata will only be removed from the plate.well metadata. The number of columns and rows will not be updated. This function is not multiprocessing safe. If you are using multiprocessing, you should use the <code>atomic_remove_image</code> method instead.</p>"},{"location":"table_specs/backend/","title":"Table Backends","text":"<p>In ngio we implemented four different table backends. Each table backend is a python class that can serialize tabular data into OME-Zarr containers.</p> <p>These backends are wrappers around existing tooling implemented in <code>anndata</code>, <code>pandas</code>, and <code>polars</code>. Currently, we provide a thin layer of metadata and table normalization to ensure that tables are serialized/deserialized in a consistent way across the different backends and across different table objects.</p> <p>In particular, we provide the metadata that describes the intended index key and type of the table for each backend.</p>"},{"location":"table_specs/backend/#anndata-backend","title":"AnnData Backend","text":"<p>AnnData is a widely used format in single-cell genomics, and can natively store complex tabular data in a Zarr group. The AnnData backend in ngio is a wrapper around the <code>anndata</code> library, which performs some table normalization for consistency and compatibility with the ngio table specifications.</p> <p>The following normalization steps are applied to each table before saving it to the AnnData backend:</p> <ul> <li>We separate the table in two parts: The floating point columns are casted to <code>float32</code> and stored as <code>X</code> in the AnnData object, while the categorical, boolean, and integer columns are stored as <code>obs</code>.</li> <li>The index column is cast to a string, and is stored in the <code>obs</code> index.</li> <li>The index column name must match the <code>index_key</code> specified in the metadata.</li> </ul> <p>AnnData backend metadata:</p> <pre><code>{\n    // Backend metadata\n    \"backend\": \"annadata\", // the backend used to store the table, e.g. \"annadata\", \"parquet\", etc..\n    \"index_key\": \"index\", // The default index key for the table, which is used to identify each row.\n    \"index_type\": \"str\", // Either \"int\" or \"str\"\n}\n</code></pre> <p>Additionally, the AnnData package will write some additional metadata to the group attributes</p> <pre><code>{\n    \"encoding-type\": \"anndata\",\n    \"encoding-version\": \"0.1.0\",\n}\n</code></pre>"},{"location":"table_specs/backend/#parquet-backend","title":"Parquet Backend","text":"<p>The Parquet backend is a high-performance columnar storage format that is widely used in big data processing. It is designed to efficiently store large datasets and can be used with various data processing frameworks. Another advantage of the Parquet backend is that it can be used lazily, meaning that the data is not loaded into memory until it is needed. This can be useful for working with large datasets that do not fit into memory.</p> <p>Parquet backend metadata:</p> <pre><code>{\n    // Backend metadata\n    \"backend\": \"parquet\", // the backend used to store the table, e.g. \"annadata\", \"parquet\", etc..\n    \"index_key\": \"index\", // The default index key for the table, which is used to identify each row.\n    \"index_type\": \"int\", // Either \"int\" or \"str\"\n}\n</code></pre> <p>The Zarr group directory will contain the Parquet file, and the metadata will be stored in the group attributes.</p> <pre><code>table.zarr          # Zarr group for the table\n\u251c\u2500\u2500 table.parquet   # Parquet file containing the table data\n\u251c\u2500\u2500 .zattrs         # Zarr group attributes containing the metadata\n\u2514\u2500\u2500 .zgroup         # Zarr group metadata\n</code></pre>"},{"location":"table_specs/backend/#csv-backend","title":"CSV Backend","text":"<p>The CSV backend is a plain text format that is widely used for tabular data. It is easy to read and write, and can be used across many different tools.</p> <p>The CSV backen in ngio follows closely the same specifications as the Parquet backend, with the following metadata:</p> <pre><code>{\n    // Backend metadata\n    \"backend\": \"csv\", // the backend used to store the table, e.g. \"annadata\", \"parquet\", etc..\n    \"index_key\": \"index\", // The default index key for the table, which is used to identify each row.\n    \"index_type\": \"int\", // Either \"int\" or \"str\"\n}\n</code></pre> <p>The Zarr group directory will contain the CSV file, and the metadata will be stored in the group attributes.</p> <pre><code>table.zarr         # Zarr group for the table\n\u251c\u2500\u2500 table.csv      # CSV file containing the table data\n\u251c\u2500\u2500 .zattrs        # Zarr group attributes containing the metadata\n\u2514\u2500\u2500 .zgroup        # Zarr group metadata\n</code></pre>"},{"location":"table_specs/backend/#json-backend","title":"JSON Backend","text":"<p>The JSON backend serializes the table data into the Zarr group attributes as a JSON object. This backend is useful for tiny tables.</p> <p>JSON backend metadata:</p> <pre><code>{\n    // Backend metadata\n    \"backend\": \"json\", // the backend used to store the table, e.g. \"annadata\", \"parquet\", etc..\n    \"index_key\": \"index\", // The default index key for the table, which is used to identify each row.\n    \"index_type\": \"int\" // Either \"int\" or \"str\"\n}\n</code></pre> <p>The table will be stored in a subgroup of the Zarr group, and the metadata will be stored in the group attributes. Storing the table in a subgroup instead of a standalone json file allows for easier access via the Zarr API.</p> <pre><code>table.zarr          # Zarr group for the table\n\u2514\u2500\u2500 table           # Zarr subgroup containing the table data\n    \u251c\u2500\u2500 .zattrs     # the json table data serialized as a JSON object\n    \u2514\u2500\u2500 .zgroup     # Zarr group metadata\n\u251c\u2500\u2500 .zattrs         # Zarr group attributes containing the metadata\n\u2514\u2500\u2500 .zgroup         # Zarr group metadata\n</code></pre>"},{"location":"table_specs/overview/","title":"Tables Overview","text":"<p>Ngio's architecture is designed to tightly integrate image and tabular data. For this purpose we developed custom specifications for serializing and deserializing tabular data into OME-Zarr containers, and semantically typed tables derived from the fractal table specification.</p>"},{"location":"table_specs/overview/#architecture","title":"Architecture","text":"<p>The ngio tables architectures is composed of three main components:</p>"},{"location":"table_specs/overview/#1-table-backends","title":"1. Table Backends","text":"<p>A backend module is a class that can serialize tabular data into OME-Zarr containers. We currently support four on-disk file formats:</p> <ul> <li>AnnData: Commonly used in single-cell genomics and was the standard table for the initial Fractal table spec.</li> <li>Parquet: A columnar storage file format optimized for large datasets.</li> <li>CSV: A simple text format for tabular data, easily human readable and writable.</li> <li>JSON: A lightweight data interchange format that both readable and efficient for small tables.</li> </ul> <p>A more detailed description of the backend module can be found in the Table Backends documentation.</p>"},{"location":"table_specs/overview/#2-in-memory-table-objects","title":"2. In-Memory Table Objects","text":"<p>These are Python objects that represent the tabular data in memory. They provide a convenient interface for manipulating and analyzing the data without needing to interact directly with the underlying file format. We support the following in-memory table objects:</p> <ul> <li>Pandas DataFrame: The most commonly used data structure for tabular data in Python.</li> <li>Polars LazyFrame: A fast DataFrame implementation that allows for lazy evaluation and efficient computation on large datasets.</li> <li>AnnData: A specialized data structure for single-cell genomics data, which goes beyond simple tabular data.</li> </ul> <p>We also provide utilities to convert between these in-memory representations in a standardized way based on the table type specifications/metadata.</p>"},{"location":"table_specs/overview/#3-table-type-specifications","title":"3. Table Type Specifications","text":"<p>These specifications define structured tables that standardize common table types used in image analysis. We have defined five table types so far:</p> <ul> <li>Generic Tables: A flexible table type that can represent any tabular data. See more in the Generic Tables documentation.</li> <li>ROI Tables: A table type specifically designed for representing Regions of Interest (ROIs) in images. See more in the ROI Tables documentation.</li> <li>Masking ROI Tables: A specialized table type for representing ROIs that are associated with specific labels in a OME-Zarr label image. See more in the Masking ROI Tables documentation.</li> <li>Feature Tables: A table type for representing features extracted from images. This table is also associated with a specific label image. See more in the Feature Tables documentation.</li> <li>Condition Tables: A table to represent experimental conditions or metadata associated with images or experiments. See more in the Condition Tables documentation.</li> </ul>"},{"location":"table_specs/overview/#tables-groups","title":"Tables Groups","text":"<p>Tables in OME-Zarr images are organized into groups of tables. Each group is saved in a Zarr group, and can be associated with a specific image or plate. The tables groups are:</p> <ul> <li>Image Tables: These tables are a sub group of the OME-Zarr image group and contain metadata or features related only to that specific image. The <code>.zarr</code> hierarchy is based on image specification in NGFF 0.4. The subgroup structure is based on the approach of the OME-Zarr <code>labels</code> group.</li> </ul> <pre><code>image.zarr        # Zarr group for a OME-Zarr image\n|\n\u251c\u2500\u2500 0             # Zarr array for multiscale level 0\n\u251c\u2500\u2500 ...\n\u251c\u2500\u2500 N             # Zarr array for multiscale level N\n|\n\u251c\u2500\u2500 labels        # Zarr subgroup with a list of labels associated to this image\n|   \u251c\u2500\u2500 label_A   # Zarr subgroup for a given label\n|   \u251c\u2500\u2500 label_B   # Zarr subgroup for a given label\n|   \u2514\u2500\u2500 ...\n|\n\u2514\u2500\u2500 tables        # Zarr subgroup with a list of tables associated to this image\n    \u251c\u2500\u2500 table_1   # Zarr subgroup for a given table\n    \u251c\u2500\u2500 table_2   # Zarr subgroup for a given table\n    \u2514\u2500\u2500 ...\n</code></pre> <ul> <li>Plate Tables: These tables are a sub group of the OME-Zarr plate group and contain metadata or features related only to that specific plate.</li> </ul> <pre><code>plate.zarr       # Zarr group for a OME-Zarr HCS plate\n|\n\u251c\u2500\u2500 A             # Row A of the plate\n|   \u251c\u2500\u2500 1         # Column 0 of row A\n|   |   \u251c\u2500\u2500 0     # Acquisition 0 of column A1\n|   |   \u251c\u2500\u2500 1     # Acquisition 1 of column A1\n|   |   \u2514\u2500\u2500 ...   # Other acquisitions of column A1\n...\n\u251c\u2500\u2500 tables        # Zarr subgroup with a list of tables associated to this plate\n|   \u251c\u2500\u2500 table_1   # Zarr subgroup for a given table\n|   \u251c\u2500\u2500 table_2   # Zarr subgroup for a given table\n|   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 ...\n</code></pre> <p>If a plate table contains per image information, the table should contain a <code>row</code>, <code>column</code>, and <code>path_in_well</code> columns.</p>"},{"location":"table_specs/overview/#tables-group-attributes","title":"Tables Group Attributes","text":"<p>The Zarr attributes of the tables group must include the key tables, pointing to the list of all tables (this simplifies discovery of tables associated to the current OME-Zarr image or plate), as in</p> <pre><code>{\n    \"tables\": [\"table_1\", \"table_2\"]\n}\n</code></pre>"},{"location":"table_specs/table_types/condition_table/","title":"Condition Table","text":"<p>A condition table is a simple table that can be used to represent experimental conditions or metadata associated with images or experiments. It is a flexible table type that can be used to store any kind of metadata related to the images or experiments.</p> <p>Example condition table:</p> Cell Type Drug Dose A Drug A 10 A Drug B 20"},{"location":"table_specs/table_types/condition_table/#specifications","title":"Specifications","text":""},{"location":"table_specs/table_types/condition_table/#v1","title":"V1","text":"<p>A condition table must include the following metadata fields in the group attributes:</p> <pre><code>{\n    // Condition table metadata\n    \"type\": \"condition_table\",\n    \"table_version\": \"1\",\n    // Backend metadata\n    \"backend\": \"csv\", // the backend used to store the table, e.g. \"annadata\", \"parquet\", etc..\n    \"index_key\": \"index\", // The default index key for the condition table, which is used to identify each row.\n    \"index_type\": \"int\" // Either \"int\" or \"str\"\n}\n</code></pre>"},{"location":"table_specs/table_types/custom_table/","title":"Add a Custom Table","text":"<p>Ngio allows users to define custom tables that can be used to store any kind of tabular data. Custom tables are flexible and can be used to represent any kind of data that does not fit into the predefined table types.</p> <p>Warning</p> <p>The library is still in the early stages and full documentation for custom tables is not yet available.</p>"},{"location":"table_specs/table_types/feature_table/","title":"Feature Tables","text":"<p>A feature table is a table type for representing per object features in an image. Each row in a feature table corresponds to a specific label in the label image.</p> <p>Feature tables can optionally include metadata to specify the type of features stored in each column:</p> <ul> <li><code>measurement</code>: A quantitative measurement of the object, such as area, perimeter, or intensity.</li> <li><code>categorical</code>: A categorical feature of the object, such as a classification label or a type.</li> <li><code>metadata</code>: Additional free-from columns that can be used to store any other information about the object, but that should not be used for analysis/classification purposes.</li> </ul> <p>These feature types inform casting of the values when serialising a table and can be used in downstream analysis to select specific subsets of features. The feature type can be explicitly specified in the feature table metadata. Alternatively, if a column is not specified, we apply the following casting rules:</p> <ul> <li>If the column contains only numeric values, it is considered a <code>measurement</code>.</li> <li>If the column contains string or boolean values, it is considered a <code>categorical</code>.</li> <li>The index column is considered a <code>categorical</code> feature.</li> </ul>"},{"location":"table_specs/table_types/feature_table/#specifications","title":"Specifications","text":""},{"location":"table_specs/table_types/feature_table/#v1","title":"V1","text":"<p>A feature table must include the following metadata fields in the group attributes:</p> <pre><code>{\n    // Feature table metadata\n    \"type\": \"feature_table\",\n    \"table_version\": \"1\",\n    \"region\": {\"path\": \"../labels/label_DAPI\"}, // Path to the label image associated with this feature table\n    // Backend metadata\n    \"backend\": \"annadata\", // the backend used to store the table, e.g. \"annadata\", \"parquet\", etc..\n    \"index_key\": \"label\", \n    \"index_type\": \"int\", // Either \"int\" or \"str\"\n}\n</code></pre> <p>Additionally, it can include feature type information such as:</p> <pre><code>{\n    \"categorical_columns\": [\n        \"label\",\n        \"cell_type\",\n    ],\n    \"measurement_columns\": [\n        \"area\",\n        \"perimeter\",\n        \"intensity_mean\",\n        \"intensity_std\"\n    ],\n    \"metadata_columns\": [\n        \"description\",\n    ],\n}\n</code></pre>"},{"location":"table_specs/table_types/generic_table/","title":"Generic Tables","text":"<p>A generic table is a flexible table type that can represent any tabular data. It is not tied to any specific domain or use case, making it suitable for a wide range of custom applications.</p> <p>Generic tables can used as a safe fallback when trying to read a table that does not match any other specific table type.</p>"},{"location":"table_specs/table_types/generic_table/#specifications","title":"Specifications","text":""},{"location":"table_specs/table_types/generic_table/#v1","title":"V1","text":"<p>A generic table should include the following metadata fields in the group attributes:</p> <pre><code>{\n    // Generic table metadata\n    \"type\": \"generic_table\",\n    \"table_version\": \"1\",\n    // Backend metadata\n    \"backend\": \"annadata\", // the backend used to store the table, e.g. \"annadata\", \"parquet\", etc..\n    \"index_key\": \"index\", // The default index key for the generic table, which is used to identify each row.\n    \"index_type\": \"int\" // Either \"int\" or \"str\"\n}\n</code></pre>"},{"location":"table_specs/table_types/masking_roi_table/","title":"Masking ROI Tables","text":"<p>A masking ROI table is a specialized table type for representing Regions of Interest (ROIs) that are associated with specific labels in a label image. Each row in a masking ROI table corresponds to a specific label in the label image.</p> <p>Masking ROI tables can be used for several purposes, such as:</p> <ul> <li>Feature extraction from specific regions in the image.</li> <li>Masking specific regions in the image for further processing. For example a masking ROI table could store the ROIs for specific tissues, and for each of these ROIs we would like to perform cell segmentation.</li> </ul>"},{"location":"table_specs/table_types/masking_roi_table/#specifications","title":"Specifications","text":""},{"location":"table_specs/table_types/masking_roi_table/#v1","title":"V1","text":"<p>A ROI table must include the following metadata fields in the group attributes:</p> <pre><code>{\n    // ROI table metadata\n    \"type\": \"masking_roi_table\",\n    \"table_version\": \"1\",\n    \"region\": {\"path\": \"../labels/label_DAPI\"}, // Path to the label image associated with this masking ROI table\n    // Backend metadata\n    \"backend\": \"annadata\", // the backend used to store the table, e.g. \"annadata\", \"parquet\", etc..\n    \"index_key\": \"label\", // The default index key for the ROI table, which is used to identify each ROI. \n    \"index_type\": \"int\", // Either \"int\" or \"str\"\n}\n</code></pre> <p>Moreover the ROI table must include the following columns:</p> <ul> <li><code>x_micrometer</code>, <code>y_micrometer</code>, <code>z_micrometer</code>: the top-left corner coordinates of the ROI in micrometers.</li> <li><code>len_x_micrometer</code>, <code>len_y_micrometer</code>, <code>len_z_micrometer</code>: the size of the ROI in micrometers along each axis.</li> </ul> <p>Additionally, each ROI can include the following optional columns: see ROI Table.</p>"},{"location":"table_specs/table_types/roi_table/","title":"ROI Table","text":"<p>A ROI table defines regions of space which are axes-aligned bounding boxes in the image space.</p> <p>ROI tables can be used for several purposes, such as:</p> <ul> <li>Storing information about the Microscope Field of View (FOV).</li> <li>Storing arbitrary regions of interest (ROIs).</li> <li>Use them as masks for other processes, such as segmentation or feature extraction.</li> </ul>"},{"location":"table_specs/table_types/roi_table/#specifications","title":"Specifications","text":""},{"location":"table_specs/table_types/roi_table/#v1","title":"V1","text":"<p>A ROI table must include the following metadata fields in the group attributes:</p> <pre><code>{\n    // ROI table metadata\n    \"type\": \"roi_table\",\n    \"table_version\": \"1\",\n    // Backend metadata\n    \"backend\": \"annadata\", // the backend used to store the table, e.g. \"annadata\", \"parquet\", etc..\n    \"index_key\": \"FieldIndex\", // The default index key for the ROI table, which is used to identify each ROI. \n    \"index_type\": \"str\", // Either \"int\" or \"str\"\n}\n</code></pre> <p>Moreover the ROI table must include the following columns:</p> <ul> <li><code>x_micrometer</code>, <code>y_micrometer</code>, <code>z_micrometer</code>: the top-left corner coordinates of the ROI in micrometers.</li> <li><code>len_x_micrometer</code>, <code>len_y_micrometer</code>, <code>len_z_micrometer</code>: the size of the ROI in micrometers along each axis.</li> </ul> <p>Additionally, each ROI can include the following optional columns:</p> <ul> <li><code>x_micrometer_original</code>, <code>y_micrometer_original</code> and <code>z_micrometer_original</code> which are the original coordinates of the ROI in micrometers. These are typically used when the data is saved in different coordinates during conversion, e.g. to avoid overwriting data from overlapping ROIs.</li> <li><code>translation_x</code>, <code>translation_y</code> and <code>translation_z</code>, which are used during registration of multiplexing acquisitions.</li> </ul> <p>The user can also add additional columns to the ROI table, but these columns will not be exposed in the ROI table API.</p>"},{"location":"tutorials/feature_extraction/","title":"Feature Extraction","text":""},{"location":"tutorials/feature_extraction/#feature-extraction","title":"Feature Extraction\u00b6","text":"<p>Coming soon! This section is still under construction. Please check back later for updates.</p>"},{"location":"tutorials/hcs_processing/","title":"HCS Processing","text":""},{"location":"tutorials/hcs_processing/#hcs-processing","title":"HCS Processing\u00b6","text":"<p>Coming soon! This section is still under construction. Please check back later for updates.</p>"},{"location":"tutorials/image_processing/","title":"Image Processing","text":""},{"location":"tutorials/image_processing/#image-processing","title":"Image Processing\u00b6","text":"<p>Coming soon! This section is still under construction. Please check back later for updates.</p>"},{"location":"tutorials/image_segmentation/","title":"Image Semgmentation","text":"In\u00a0[1]: Copied! <pre># Setup a simple segmentation function\nimport numpy as np\nimport skimage\n\n\ndef otsu_threshold_segmentation(image: np.ndarray, max_label: int) -&gt; np.ndarray:\n    \"\"\"Simple segmentation using Otsu thresholding.\"\"\"\n    threshold = skimage.filters.threshold_otsu(image)\n    binary = image &gt; threshold\n    label_image = skimage.measure.label(binary)\n    label_image += max_label\n    label_image = np.where(binary, label_image, 0)\n    return label_image\n</pre> # Setup a simple segmentation function import numpy as np import skimage   def otsu_threshold_segmentation(image: np.ndarray, max_label: int) -&gt; np.ndarray:     \"\"\"Simple segmentation using Otsu thresholding.\"\"\"     threshold = skimage.filters.threshold_otsu(image)     binary = image &gt; threshold     label_image = skimage.measure.label(binary)     label_image += max_label     label_image = np.where(binary, label_image, 0)     return label_image In\u00a0[2]: Copied! <pre>from pathlib import Path\n\nfrom ngio import open_ome_zarr_container\nfrom ngio.utils import download_ome_zarr_dataset\n\n# Download the dataset\ndownload_dir = Path(\".\").absolute().parent.parent / \"data\"\nhcs_path = download_ome_zarr_dataset(\"CardiomyocyteTiny\", download_dir=download_dir)\nimage_path = hcs_path / \"B\" / \"03\" / \"0\"\n\n# Open the ome-zarr container\nome_zarr = open_ome_zarr_container(image_path)\n</pre> from pathlib import Path  from ngio import open_ome_zarr_container from ngio.utils import download_ome_zarr_dataset  # Download the dataset download_dir = Path(\".\").absolute().parent.parent / \"data\" hcs_path = download_ome_zarr_dataset(\"CardiomyocyteTiny\", download_dir=download_dir) image_path = hcs_path / \"B\" / \"03\" / \"0\"  # Open the ome-zarr container ome_zarr = open_ome_zarr_container(image_path) <pre>Unzipping contents of '/home/runner/work/ngio/ngio/data/20200812-CardiomyocyteDifferentiation14-Cycle1-tiny.zarr.zip' to '/home/runner/work/ngio/ngio/data/tmp'\n</pre> In\u00a0[3]: Copied! <pre># First we will need the image object and the FOVs table\nimage = ome_zarr.get_image()\nroi_table = ome_zarr.get_table(\"FOV_ROI_table\", check_type=\"roi_table\")\n\n# Second we need to derive a new label image to use as target for the segmentation\n\nlabel = ome_zarr.derive_label(\"new_label\", overwrite=True)\n\nmax_label = 0  # We will use this to avoid label collisions\nfor roi in roi_table.rois():\n    image_data = image.get_roi(roi=roi, c=0)  # Get the image data for the ROI\n\n    image_data = image_data.squeeze()  # Remove the channel dimension\n    roi_segmentation = otsu_threshold_segmentation(\n        image_data, max_label\n    )  # Segment the image\n\n    max_label = roi_segmentation.max()  # Get the max label for the next iteration\n\n    label.set_roi(\n        roi=roi, patch=roi_segmentation\n    )  # Write the segmentation to the label image\n</pre> # First we will need the image object and the FOVs table image = ome_zarr.get_image() roi_table = ome_zarr.get_table(\"FOV_ROI_table\", check_type=\"roi_table\")  # Second we need to derive a new label image to use as target for the segmentation  label = ome_zarr.derive_label(\"new_label\", overwrite=True)  max_label = 0  # We will use this to avoid label collisions for roi in roi_table.rois():     image_data = image.get_roi(roi=roi, c=0)  # Get the image data for the ROI      image_data = image_data.squeeze()  # Remove the channel dimension     roi_segmentation = otsu_threshold_segmentation(         image_data, max_label     )  # Segment the image      max_label = roi_segmentation.max()  # Get the max label for the next iteration      label.set_roi(         roi=roi, patch=roi_segmentation     )  # Write the segmentation to the label image <pre>/tmp/ipykernel_2605/4229125940.py:3: DeprecationWarning: The 'check_type' argument is deprecated, and will be removed in ngio=0.3. Use 'get_table_as' instead or one of the type specific get_*table() methods.\n  roi_table = ome_zarr.get_table(\"FOV_ROI_table\", check_type=\"roi_table\")\n</pre> In\u00a0[4]: Copied! <pre>label.consolidate()\n</pre> label.consolidate() In\u00a0[5]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.colors import ListedColormap\n\nrand_cmap = np.random.rand(1000, 3)\nrand_cmap[0] = 0\nrand_cmap = ListedColormap(rand_cmap)\n\nfig, axs = plt.subplots(2, 1, figsize=(8, 4))\naxs[0].set_title(\"Original image\")\naxs[0].imshow(image.get_array(c=0, z=1).squeeze(), cmap=\"gray\")\naxs[1].set_title(\"Final segmentation\")\naxs[1].imshow(label.get_array(z=1).squeeze(), cmap=rand_cmap)\nfor ax in axs:\n    ax.axis(\"off\")\nplt.tight_layout()\nplt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np from matplotlib.colors import ListedColormap  rand_cmap = np.random.rand(1000, 3) rand_cmap[0] = 0 rand_cmap = ListedColormap(rand_cmap)  fig, axs = plt.subplots(2, 1, figsize=(8, 4)) axs[0].set_title(\"Original image\") axs[0].imshow(image.get_array(c=0, z=1).squeeze(), cmap=\"gray\") axs[1].set_title(\"Final segmentation\") axs[1].imshow(label.get_array(z=1).squeeze(), cmap=rand_cmap) for ax in axs:     ax.axis(\"off\") plt.tight_layout() plt.show()"},{"location":"tutorials/image_segmentation/#image-semgmentation","title":"Image Semgmentation\u00b6","text":"<p>This is a minimal tutorial on how to use ngio for image segmentation.</p>"},{"location":"tutorials/image_segmentation/#step-1-setup","title":"Step 1: Setup\u00b6","text":"<p>We will first implement a very simple function to segment an image. We will use skimage to do this.</p>"},{"location":"tutorials/image_segmentation/#step-2-open-the-omezarr-container","title":"Step 2: Open the OmeZarr container\u00b6","text":""},{"location":"tutorials/image_segmentation/#step-3-segment-the-image","title":"Step 3: Segment the image\u00b6","text":"<p>For this example, we will not segment the image all at once. Instead we will iterate over the image FOVs and segment them one by one.</p>"},{"location":"tutorials/image_segmentation/#step-4-consolidate-the-segmentation","title":"Step 4: Consolidate the segmentation\u00b6","text":"<p>The <code>new_label</code> has data only at a single resolution lebel. To consolidate the segmentation to all other levels we will need to call the <code>consolidate</code> method.</p>"},{"location":"tutorials/image_segmentation/#plot-the-segmentation","title":"Plot the segmentation\u00b6","text":""}]}